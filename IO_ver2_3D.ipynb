{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nibabel in d:\\sofwares\\anaconda\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.13 in d:\\sofwares\\anaconda\\lib\\site-packages (from nibabel) (1.18.1)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\sofwares\\anaconda\\lib\\site-packages (from nibabel) (20.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\sofwares\\anaconda\\lib\\site-packages (from packaging>=14.3->nibabel) (2.4.6)\n",
      "Requirement already satisfied: six in d:\\sofwares\\anaconda\\lib\\site-packages (from packaging>=14.3->nibabel) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib \n",
    "! pip install nibabel --user\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from nibabel.testing import data_path\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2\n",
    "#! pip install cv2 --user\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread\n",
    "from skimage.color import gray2rgb\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision as tv\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data as DT\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\\\Master_courses\\\\Praktikum\\\\Datasets\\\\Mayank\\\\hecktor_nii\\\\'\n",
    "divisor=128.0\n",
    "\n",
    "files = [f for f in glob.glob(path + \"**/*_ct_gtvt.nii.gz\", recursive=True)]\n",
    "labels_lenght=list()\n",
    "image_list=list()\n",
    "label_list=list()\n",
    "\n",
    "for idx,f in enumerate(files):\n",
    "    SEG_img = nib.load(f)\n",
    "    SEG_data = SEG_img.get_fdata()\n",
    "    prev_shape=SEG_data.shape\n",
    "    new_shape=[divisor/x for x in prev_shape]\n",
    "    SEG_data = zoom(SEG_data, new_shape,order=1).astype(np.float)\n",
    "    label_list.append(SEG_data)\n",
    "    path=f[:-15]+'_pt.nii.gz'\n",
    "    im=nib.load(path)\n",
    "    PET_data=im.get_fdata()\n",
    "    prev_shape=PET_data.shape\n",
    "    new_shape=[divisor/x for x in prev_shape]\n",
    "    PET_data = zoom(PET_data, new_shape,order=1).astype(np.float)\n",
    "    image_list.append(PET_data)\n",
    "    #print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_list) # number of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 32 # patch size\n",
    "stride = 32 # patch stride\n",
    "\n",
    "all_labels=list()\n",
    "for seg_img in label_list:\n",
    "    patches = torch.from_numpy(seg_img).unfold(0, size, stride).unfold(1, size, stride).unfold(2, size, stride)\n",
    "    reshaped_patches=patches.reshape(-1,size,size,size)\n",
    "    labels_=torch.sum(reshaped_patches , dim=(1,2,3)).bool().int().numpy()\n",
    "    all_labels.append(labels_)\n",
    "all_labels=np.asarray(all_labels).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[:64] # labels of one patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_PET_patches=list()\n",
    "for PET_img in image_list:\n",
    "    patches = torch.from_numpy(PET_img).unfold(0, size, stride).unfold(1, size, stride).unfold(2, size, stride)\n",
    "    reshaped_patches=patches.reshape(-1,size,size,size).numpy()\n",
    "    all_PET_patches.append(reshaped_patches)\n",
    "all_PET_patches=np.asarray(all_PET_patches).reshape(-1,size,size,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12928, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print (all_PET_patches.shape) #number of patches * pactch dim1 * patch dim2 * patch dim3\n",
    "patch_mean=np.mean(all_PET_patches , axis=0)\n",
    "patch_std=np.std(all_PET_patches , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "in_channel = 32\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HN_Dataset(Dataset):\n",
    "    def __init__(self, transform=tv.transforms.Compose(tv.transforms.ToTensor())):\n",
    "        self.labels = all_labels\n",
    "        self.images=all_PET_patches\n",
    "        self.transform = transform\n",
    "        self.mode='None'\n",
    "    def __len__(self):\n",
    "        return len(all_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label= torch.from_numpy(np.asarray(self.labels[idx]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            PET_data = self.transform(self.images[idx])\n",
    "        return (PET_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target train 0/1: 11595/405\n",
      "batch index 0, 0/1: 30/34\n",
      "batch index 1, 0/1: 32/32\n",
      "batch index 2, 0/1: 34/30\n",
      "batch index 3, 0/1: 39/25\n",
      "batch index 4, 0/1: 30/34\n",
      "batch index 5, 0/1: 29/35\n",
      "batch index 6, 0/1: 32/32\n",
      "batch index 7, 0/1: 34/30\n",
      "batch index 8, 0/1: 28/36\n",
      "batch index 9, 0/1: 38/26\n",
      "batch index 10, 0/1: 31/33\n",
      "batch index 11, 0/1: 30/34\n",
      "batch index 12, 0/1: 38/26\n",
      "batch index 13, 0/1: 38/26\n",
      "batch index 14, 0/1: 36/28\n",
      "batch index 15, 0/1: 27/37\n",
      "batch index 16, 0/1: 30/34\n",
      "batch index 17, 0/1: 36/28\n",
      "batch index 18, 0/1: 28/36\n",
      "batch index 19, 0/1: 29/35\n",
      "batch index 20, 0/1: 31/33\n",
      "batch index 21, 0/1: 39/25\n",
      "batch index 22, 0/1: 30/34\n",
      "batch index 23, 0/1: 32/32\n",
      "batch index 24, 0/1: 37/27\n",
      "batch index 25, 0/1: 41/23\n",
      "batch index 26, 0/1: 34/30\n",
      "batch index 27, 0/1: 29/35\n",
      "batch index 28, 0/1: 26/38\n",
      "batch index 29, 0/1: 40/24\n",
      "batch index 30, 0/1: 33/31\n",
      "batch index 31, 0/1: 43/21\n",
      "batch index 32, 0/1: 33/31\n",
      "batch index 33, 0/1: 31/33\n",
      "batch index 34, 0/1: 38/26\n",
      "batch index 35, 0/1: 34/30\n",
      "batch index 36, 0/1: 33/31\n",
      "batch index 37, 0/1: 29/35\n",
      "batch index 38, 0/1: 30/34\n",
      "batch index 39, 0/1: 35/29\n",
      "batch index 40, 0/1: 28/36\n",
      "batch index 41, 0/1: 28/36\n",
      "batch index 42, 0/1: 34/30\n",
      "batch index 43, 0/1: 36/28\n",
      "batch index 44, 0/1: 32/32\n",
      "batch index 45, 0/1: 37/27\n",
      "batch index 46, 0/1: 31/33\n",
      "batch index 47, 0/1: 37/27\n",
      "batch index 48, 0/1: 30/34\n",
      "batch index 49, 0/1: 37/27\n",
      "batch index 50, 0/1: 29/35\n",
      "batch index 51, 0/1: 32/32\n",
      "batch index 52, 0/1: 38/26\n",
      "batch index 53, 0/1: 29/35\n",
      "batch index 54, 0/1: 36/28\n",
      "batch index 55, 0/1: 31/33\n",
      "batch index 56, 0/1: 32/32\n",
      "batch index 57, 0/1: 28/36\n",
      "batch index 58, 0/1: 31/33\n",
      "batch index 59, 0/1: 31/33\n",
      "batch index 60, 0/1: 35/29\n",
      "batch index 61, 0/1: 33/31\n",
      "batch index 62, 0/1: 35/29\n",
      "batch index 63, 0/1: 30/34\n",
      "batch index 64, 0/1: 42/22\n",
      "batch index 65, 0/1: 28/36\n",
      "batch index 66, 0/1: 33/31\n",
      "batch index 67, 0/1: 30/34\n",
      "batch index 68, 0/1: 30/34\n",
      "batch index 69, 0/1: 35/29\n",
      "batch index 70, 0/1: 32/32\n",
      "batch index 71, 0/1: 36/28\n",
      "batch index 72, 0/1: 23/41\n",
      "batch index 73, 0/1: 33/31\n",
      "batch index 74, 0/1: 29/35\n",
      "batch index 75, 0/1: 37/27\n",
      "batch index 76, 0/1: 35/29\n",
      "batch index 77, 0/1: 32/32\n",
      "batch index 78, 0/1: 36/28\n",
      "batch index 79, 0/1: 34/30\n",
      "batch index 80, 0/1: 32/32\n",
      "batch index 81, 0/1: 31/33\n",
      "batch index 82, 0/1: 31/33\n",
      "batch index 83, 0/1: 33/31\n",
      "batch index 84, 0/1: 40/24\n",
      "batch index 85, 0/1: 34/30\n",
      "batch index 86, 0/1: 33/31\n",
      "batch index 87, 0/1: 26/38\n",
      "batch index 88, 0/1: 29/35\n",
      "batch index 89, 0/1: 40/24\n",
      "batch index 90, 0/1: 37/27\n",
      "batch index 91, 0/1: 30/34\n",
      "batch index 92, 0/1: 33/31\n",
      "batch index 93, 0/1: 34/30\n",
      "batch index 94, 0/1: 31/33\n",
      "batch index 95, 0/1: 32/32\n",
      "batch index 96, 0/1: 35/29\n",
      "batch index 97, 0/1: 34/30\n",
      "batch index 98, 0/1: 33/31\n",
      "batch index 99, 0/1: 32/32\n",
      "batch index 100, 0/1: 41/23\n",
      "batch index 101, 0/1: 33/31\n",
      "batch index 102, 0/1: 30/34\n",
      "batch index 103, 0/1: 28/36\n",
      "batch index 104, 0/1: 28/36\n",
      "batch index 105, 0/1: 33/31\n",
      "batch index 106, 0/1: 33/31\n",
      "batch index 107, 0/1: 30/34\n",
      "batch index 108, 0/1: 32/32\n",
      "batch index 109, 0/1: 31/33\n",
      "batch index 110, 0/1: 31/33\n",
      "batch index 111, 0/1: 30/34\n",
      "batch index 112, 0/1: 33/31\n",
      "batch index 113, 0/1: 34/30\n",
      "batch index 114, 0/1: 31/33\n",
      "batch index 115, 0/1: 29/35\n",
      "batch index 116, 0/1: 31/33\n",
      "batch index 117, 0/1: 32/32\n",
      "batch index 118, 0/1: 32/32\n",
      "batch index 119, 0/1: 27/37\n",
      "batch index 120, 0/1: 32/32\n",
      "batch index 121, 0/1: 24/40\n",
      "batch index 122, 0/1: 32/32\n",
      "batch index 123, 0/1: 32/32\n",
      "batch index 124, 0/1: 19/45\n",
      "batch index 125, 0/1: 39/25\n",
      "batch index 126, 0/1: 43/21\n",
      "batch index 127, 0/1: 30/34\n",
      "batch index 128, 0/1: 25/39\n",
      "batch index 129, 0/1: 35/29\n",
      "batch index 130, 0/1: 30/34\n",
      "batch index 131, 0/1: 26/38\n",
      "batch index 132, 0/1: 30/34\n",
      "batch index 133, 0/1: 31/33\n",
      "batch index 134, 0/1: 32/32\n",
      "batch index 135, 0/1: 31/33\n",
      "batch index 136, 0/1: 34/30\n",
      "batch index 137, 0/1: 33/31\n",
      "batch index 138, 0/1: 34/30\n",
      "batch index 139, 0/1: 37/27\n",
      "batch index 140, 0/1: 23/41\n",
      "batch index 141, 0/1: 26/38\n",
      "batch index 142, 0/1: 30/34\n",
      "batch index 143, 0/1: 33/31\n",
      "batch index 144, 0/1: 30/34\n",
      "batch index 145, 0/1: 27/37\n",
      "batch index 146, 0/1: 27/37\n",
      "batch index 147, 0/1: 31/33\n",
      "batch index 148, 0/1: 32/32\n",
      "batch index 149, 0/1: 33/31\n",
      "batch index 150, 0/1: 36/28\n",
      "batch index 151, 0/1: 27/37\n",
      "batch index 152, 0/1: 36/28\n",
      "batch index 153, 0/1: 30/34\n",
      "batch index 154, 0/1: 38/26\n",
      "batch index 155, 0/1: 20/44\n",
      "batch index 156, 0/1: 26/38\n",
      "batch index 157, 0/1: 32/32\n",
      "batch index 158, 0/1: 35/29\n",
      "batch index 159, 0/1: 33/31\n",
      "batch index 160, 0/1: 28/36\n",
      "batch index 161, 0/1: 38/26\n",
      "batch index 162, 0/1: 25/39\n",
      "batch index 163, 0/1: 40/24\n",
      "batch index 164, 0/1: 33/31\n",
      "batch index 165, 0/1: 32/32\n",
      "batch index 166, 0/1: 27/37\n",
      "batch index 167, 0/1: 29/35\n",
      "batch index 168, 0/1: 37/27\n",
      "batch index 169, 0/1: 38/26\n",
      "batch index 170, 0/1: 37/27\n",
      "batch index 171, 0/1: 35/29\n",
      "batch index 172, 0/1: 32/32\n",
      "batch index 173, 0/1: 31/33\n",
      "batch index 174, 0/1: 28/36\n",
      "batch index 175, 0/1: 26/38\n",
      "batch index 176, 0/1: 34/30\n",
      "batch index 177, 0/1: 36/28\n",
      "batch index 178, 0/1: 38/26\n",
      "batch index 179, 0/1: 26/38\n",
      "batch index 180, 0/1: 26/38\n",
      "batch index 181, 0/1: 35/29\n",
      "batch index 182, 0/1: 31/33\n",
      "batch index 183, 0/1: 31/33\n",
      "batch index 184, 0/1: 35/29\n",
      "batch index 185, 0/1: 25/39\n",
      "batch index 186, 0/1: 34/30\n",
      "batch index 187, 0/1: 20/12\n"
     ]
    }
   ],
   "source": [
    "split_threshold=12000  # to split test and train set\n",
    "\n",
    "print ('target train 0/1: {}/{}'.format(\n",
    "    len(np.where(all_labels[:split_threshold] == 0)[0]), len(np.where(all_labels[:split_threshold] == 1)[0])))\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(all_labels[:split_threshold] == t)[0]) for t in np.unique(all_labels[:split_threshold])])\n",
    "weight = ([1. ,1.]) / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in all_labels[:split_threshold]])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\"\"\"\n",
    "target = torch.from_numpy(all_labels[:70000]).int()\n",
    "data=torch.from_numpy(all_PET_patches[:70000])\n",
    "train_set = torch.utils.data.TensorDataset(data, target)\n",
    "target = torch.from_numpy(all_labels[70000:]).int()\n",
    "data=torch.from_numpy(all_PET_patches[70000:])\n",
    "test_set = torch.utils.data.TensorDataset(data, target)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Load Data\n",
    "\n",
    "idx = list(range(len(all_labels)))\n",
    "\n",
    "dataset = HN_Dataset( transform=tv.transforms.ToTensor())\n",
    "train_set = DT.Subset(dataset, idx[:split_threshold])\n",
    "test_set = DT.Subset(dataset, idx[split_threshold:])\n",
    "\n",
    "#train_set, test_set = torch.utils.data.random_split(dataset, [70000, 6800])\n",
    "\n",
    "train_set.transform= tv.transforms.Compose([tv.transforms.RandomHorizontalFlip(),\n",
    "                                            tv.transforms.ToTensor(),\n",
    "                                            tv.transforms.Normalize(patch_mean, patch_std)])\n",
    "\n",
    "#tv.transforms.RandomRotation(180), tv.transforms.RandomVerticalFlip(),tv.transforms.RandomVerticalFlip(),\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, sampler= sampler)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    print (\"batch index {}, 0/1: {}/{}\".format(\n",
    "        i,\n",
    "        len(np.where(target.numpy() == 0)[0]),\n",
    "        len(np.where(target.numpy() == 1)[0])))\n",
    "    #print ((data[1].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataIter = iter(test_loader)\n",
    "imgs, labels = dataIter.next()\n",
    "\n",
    "\n",
    "print (\"labels\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=16, num_classes=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=16,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(2, 2),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=8,\n",
    "            kernel_size=(3 ,3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=8,\n",
    "            kernel_size=(3 ,3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=4,\n",
    "            kernel_size=(3 ,3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            kernel_size=(3 ,3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(4 * 2 * 2, num_classes)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.initialize_weights()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        #x = F.relu(self.conv5(x))\n",
    "        #x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x=self.sig(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint_3D_V2.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 00:16:35\n",
      "Cost at epoch 0 is 0.42573357642965115\n",
      "Current Time = 00:16:35\n",
      "=> Saving checkpoint\n",
      "Cost at epoch 4 is 0.13593927494428576\n",
      "Current Time = 00:17:16\n",
      "=> Saving checkpoint\n",
      "Cost at epoch 8 is 0.10998262107332653\n",
      "Current Time = 00:17:56\n",
      "=> Saving checkpoint\n",
      "Cost at epoch 12 is 0.06158317745653318\n",
      "Current Time = 00:18:37\n",
      "=> Saving checkpoint\n",
      "Cost at epoch 16 is 0.056205892229472545\n",
      "Current Time = 00:19:18\n",
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize network\n",
    "model = CNN().to(device)\n",
    "#model = model.float()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "#criterion=nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "load_module=False\n",
    "print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\")) \n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    now = datetime.now()\n",
    "    if load_module:\n",
    "        # load checkpoint\n",
    "        load_checkpoint(torch.load(\"my_checkpoint_3D_V2.pth.tar\"), model, optimizer)\n",
    "        load_module=False\n",
    "    losses = []\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        data=data.float()\n",
    "        scores = model(data)\n",
    "        targets=targets.float()\n",
    "        scores=scores.view(-1)\n",
    "\n",
    "        loss = criterion(scores, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    if (epoch % 4==0):\n",
    "        print(f\"Cost at epoch {epoch} is {np.sum(losses)/len(losses)}\")\n",
    "        print(\"Current Time =\", now.strftime(\"%H:%M:%S\")) \n",
    "        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        # save checkpoint\n",
    "        save_checkpoint(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training to see how good our model is\n",
    "def check_accuracy(loader, model, test_set):\n",
    "    true_positives,false_positives,true_negatives,false_negatives=0,0,0,0\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    j=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x=x.float()\n",
    "            scores = model(x)\n",
    "            predictions=torch.round(scores).view(-1)\n",
    "            \n",
    "            \n",
    "            #confusion matrix\n",
    "            confusion_vector = y.cpu() / predictions.cpu()\n",
    "            true_positives =true_positives+ torch.sum(confusion_vector == 1).item()\n",
    "            false_positives =false_positives+ torch.sum(confusion_vector == float('inf')).item()\n",
    "            true_negatives =true_negatives+ torch.sum(torch.isnan(confusion_vector)).item()\n",
    "            false_negatives =false_negatives+ torch.sum(confusion_vector == 0).item()\n",
    "            \n",
    "            \n",
    "            if test_set:\n",
    "                #imshow(x.cpu())\n",
    "                #print (\"labels:       \",(y.cpu()))\n",
    "                #print (\"predictions:  \",(predictions.cpu()))\n",
    "                \n",
    "                \n",
    "                print (\"labels:       \",np.ravel(y.cpu().numpy()))\n",
    "                print (\"predictions:  \",np.ravel((predictions.cpu().numpy()).astype(np.int)))\n",
    "                np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "                print (np.ravel(scores.cpu().numpy()))\n",
    "                #output=scores\n",
    "                #test_set=False\n",
    "\n",
    "            num_correct += (predictions.view(-1) == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        percision= true_positives/(true_positives+false_positives)\n",
    "        recall=true_positives/(true_positives+false_negatives)\n",
    "        F1score= 2*(percision*recall)/(percision+recall)\n",
    "        print(f\"Got {num_correct} / {num_samples} with ratio of {float(num_correct)/float(num_samples)*100:.2f}\")\n",
    "        print (\"true_positives\",true_positives)\n",
    "        print (\"false_positives\",false_positives)\n",
    "        print (\"true_negatives\",true_negatives)\n",
    "        print (\"false_negatives\",false_negatives)\n",
    "        print (\"F1score\",F1score)\n",
    "\n",
    "        \n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on Training Set\n",
      "Got 11876 / 12000 with ratio of 98.97\n",
      "true_positives 5993\n",
      "false_positives 0\n",
      "true_negatives 5883\n",
      "false_negatives 124\n",
      "F1score 0.9897605284888522\n",
      "Checking accuracy on Test Set\n",
      "Got 893 / 928 with ratio of 96.23\n",
      "true_positives 30\n",
      "false_positives 4\n",
      "true_negatives 863\n",
      "false_negatives 31\n",
      "F1score 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking accuracy on Training Set\")\n",
    "check_accuracy(train_loader, model ,test_set=False)\n",
    "\n",
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(test_loader, model, test_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on Test Set\n",
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.001  0.323  0.004  0.000  0.952  0.000  0.999  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.004  0.000  0.000\n",
      "  0.003  0.375  0.152  0.000  0.000  0.005  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.001  0.005  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001\n",
      "  0.000  0.000  0.089  0.881  0.831  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.002  0.000  0.000  0.555  0.906  0.990  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.008  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.016  0.547  0.977  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.000  0.000  0.000  0.000  0.569  0.942  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.982  0.966  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.000  0.000  0.000  0.207  0.537  0.992  0.000  0.000  0.014\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.009  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.761  0.963  0.964  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.000  0.000  0.000  0.000  0.958  0.674  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001\n",
      "  0.000  0.000  0.060  0.983  0.379  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.001  0.000  0.000  0.000  0.994  0.997  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.003  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001\n",
      "  0.000  0.000  0.000  0.935  0.990  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.001  0.000  0.000  0.000  0.988  0.995  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.005  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.002  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.002\n",
      "  0.000  0.000  0.007  0.824  0.952  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.003  0.000  0.000  0.000  0.936  0.710  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.293  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.003\n",
      "  0.000  0.000  0.000  0.983  0.952  0.000  0.002  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.001  0.000  0.000  0.000  0.977  1.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.001  0.000  0.000\n",
      "  0.000  0.038  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.021  0.000  0.000\n",
      "  0.001  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001\n",
      "  0.000  0.000  0.273  0.878  0.965  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.001  0.000  0.000  0.000  0.646  0.930  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.024  0.277  0.002  0.000  0.005  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.929  0.975  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.003  0.000  0.000  0.000  0.994  0.990  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.002\n",
      "  0.000  0.000  0.000  0.614  0.975  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.000  0.000  0.000  0.000  0.964  0.984  0.000  0.000  0.002\n",
      "  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.007  0.000  0.000  0.000  0.003  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.001  0.000  0.003  0.000  0.645  0.871  0.377  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.456  0.000\n",
      "  0.000  0.000  0.003  0.023  0.775  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.787  0.000  0.000  0.000  0.000  0.000  0.839  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.067  0.002  0.002  0.000\n",
      "  0.213  0.146  0.562  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.002  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.752  0.981  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.000  0.000  0.000  0.000  0.652  0.889  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.000  0.002  0.000\n",
      "  0.000  0.003  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001  0.000  0.000\n",
      "  0.003  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.835  0.938  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000]\n",
      "labels:        [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predictions:   [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.000  0.000  0.000  0.000  0.000  0.859  0.945  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000  0.000  0.000  0.006  0.001  0.000  0.000  0.000  0.000\n",
      "  0.000  0.000]\n",
      "Got 893 / 928 with accuracy 96.23\n",
      "true_positives 30\n",
      "false_positives 4\n",
      "true_negatives 863\n",
      "false_negatives 31\n",
      "F1score 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(test_loader, model, test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
