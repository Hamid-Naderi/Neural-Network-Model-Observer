{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib \n",
    "#! pip install nibabel --user\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from nibabel.testing import data_path\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2\n",
    "#! pip install cv2 --user\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread\n",
    "from skimage.color import gray2rgb\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision as tv\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data as DT\n",
    "from skimage.transform import radon , iradon\n",
    "%matplotlib inline\n",
    "!pip install livelossplot --quiet\n",
    "from livelossplot import PlotLosses\n",
    "from sklearn.metrics import r2_score , mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###pad function to resize all images to specific size (here: 64*64*64)\n",
    "\n",
    "def pad(img_,size_):\n",
    "    dif=size_-img_.shape[0]\n",
    "    if dif%2==0:\n",
    "        img_=np.pad(img_,((dif//2,dif//2),(0,0),(0,0)),'edge')\n",
    "    else:\n",
    "        img_=np.pad(img_,(((dif-1)//2,(dif+1)//2),(0,0),(0,0)),'edge')\n",
    "    dif=size_-img_.shape[1]\n",
    "    if dif%2==0:\n",
    "        img_=np.pad(img_,((0,0),(dif//2,dif//2),(0,0)),'edge')\n",
    "    else:\n",
    "        img_=np.pad(img_,((0,0),((dif-1)//2,(dif+1)//2),(0,0)),'edge')\n",
    "    dif=size_-img_.shape[2]\n",
    "    if dif%2==0:\n",
    "        img_=np.pad(img_,((0,0),(0,0),(dif//2,dif//2)),'edge')\n",
    "    else:\n",
    "        img_=np.pad(img_,((0,0),(0,0),((dif-1)//2,(dif+1)//2)),'edge')\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to save the model\n",
    "def save_checkpoint(state, filename=\"my_checkpoint_V3.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "#method to load the saved checkpoint\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all PET and segmentation images(these images are already resampled and have the size of [almost] 64*64*64)\n",
    "#and add padding if needed\n",
    "\n",
    "path = 'D:\\\\Master_courses\\\\Praktikum\\\\challenge\\\\hecktor\\\\data\\\\resampled\\\\'\n",
    "\n",
    "files = [f for f in glob.glob(path + \"**/*_ct_gtvt.nii.gz\", recursive=True)]\n",
    "labels_lenght=list()\n",
    "image_list=list()\n",
    "label_list=list()\n",
    "\n",
    "for idx,f in enumerate(files):\n",
    "    SEG_img = nib.load(f)\n",
    "    SEG_data = SEG_img.get_fdata()\n",
    "    if SEG_data.shape!=(64,64,64):\n",
    "        SEG_data=pad(SEG_data,64)\n",
    "    label_list.append(SEG_data)\n",
    "    path=f[:-15]+'_pt.nii.gz'\n",
    "    im=nib.load(path)\n",
    "    PET_data=im.get_fdata()\n",
    "    if PET_data.shape!=(64,64,64):\n",
    "        PET_data=pad(PET_data,64)\n",
    "    image_list.append(PET_data)\n",
    "    #print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_list) # number of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print (image_list[0].shape) # shape of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make label patch of 16*16*16\n",
    "\n",
    "size = 16 # patch size\n",
    "stride = 16 # patch stride\n",
    "\n",
    "all_labels=list()\n",
    "for seg_img in label_list:\n",
    "    positive_cells_number=np.sum(seg_img)\n",
    "    #print(\"positive_cells_number\",positive_cells_number)\n",
    "    patches = torch.from_numpy(seg_img).unfold(0, size, stride).unfold(1, size, stride).unfold(2, size, stride)\n",
    "    reshaped_patches=patches.reshape(-1,size,size,size)\n",
    "    labels_=(torch.sum(reshaped_patches , dim=(1,2,3)).int().numpy())/positive_cells_number\n",
    "    #print (\"labels_\", labels_)\n",
    "    all_labels.append(labels_)\n",
    "all_labels=np.asarray(all_labels).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00208671, 0.008115  , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00139114,\n",
       "       0.        , 0.        , 0.11245073, 0.69116624, 0.03524229,\n",
       "       0.        , 0.01738929, 0.10479944, 0.00092743, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.02643172,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[:64] # labels of one patient as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make image patch of 16*16*16\n",
    "\n",
    "all_PET_patches=list()\n",
    "for PET_img in image_list:\n",
    "    patches = torch.from_numpy(PET_img).unfold(0, size, stride).unfold(1, size, stride).unfold(2, size, stride)\n",
    "    reshaped_patches=patches.reshape(-1,size,size,size).numpy()\n",
    "    all_PET_patches.append(reshaped_patches)\n",
    "all_PET_patches=np.asarray(all_PET_patches).reshape(-1,size,size,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 16, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "print (all_PET_patches.shape) #number of patches * pactch dim1 * patch dim2 * patch dim3\n",
    "patch_mean=np.mean(all_PET_patches , axis=0)\n",
    "patch_std=np.std(all_PET_patches , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HN_Dataset(Dataset):\n",
    "    def __init__(self, transform=tv.transforms.Compose(tv.transforms.ToTensor())):\n",
    "        self.labels = all_labels\n",
    "        self.images=all_PET_patches\n",
    "        self.transform = transform\n",
    "        self.mode='None'\n",
    "    def __len__(self):\n",
    "        return len(all_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label= torch.from_numpy(np.asarray(self.labels[idx]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            PET_data = self.transform(self.images[idx])\n",
    "        return (PET_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_threshold=10000  # to split test and train set (10000 out of 12800 for train set)\n",
    "batch_size = 64\n",
    "\n",
    "# Load Data\n",
    "\n",
    "idx = list(range(len(all_labels)))\n",
    "\n",
    "dataset = HN_Dataset( transform=tv.transforms.ToTensor())\n",
    "train_set = DT.Subset(dataset, idx[:split_threshold])\n",
    "test_set = DT.Subset(dataset, idx[split_threshold:])\n",
    "\n",
    "#train_set, test_set = torch.utils.data.random_split(dataset, [10000, 2800])\n",
    "\n",
    "train_set.transform= tv.transforms.Compose([#tv.transforms.RandomHorizontalFlip(),\n",
    "                                            #tv.transforms.RandomVerticalFlip(),\n",
    "                                            tv.transforms.ToTensor(),\n",
    "                                            tv.transforms.Normalize(patch_mean, patch_std)\n",
    "                                           ])\n",
    "\n",
    "#tv.transforms.RandomRotation(180),\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_conv2d(nn.Module):\n",
    "    def __init__(self, in_channels=16, num_classes=1):\n",
    "        super(CNN_conv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=(5,5),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=16,\n",
    "            kernel_size=(3 ,3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=8,\n",
    "            kernel_size=(3 ,3),\n",
    "            stride=(1, 1),\n",
    "            #padding=(1, 1),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, num_classes)\n",
    "        self.fc2 = nn.Linear(8, num_classes)\n",
    "        \n",
    "        self.sig=nn.Sigmoid()\n",
    "        \n",
    "        self.BatchNorm64=nn.BatchNorm2d(64)\n",
    "        self.BatchNorm32=nn.BatchNorm2d(32)\n",
    "        self.BatchNorm8=nn.BatchNorm2d(8)\n",
    "        self.BatchNorm128=nn.BatchNorm2d(128)\n",
    "        self.BatchNorm16=nn.BatchNorm2d(16)\n",
    "        self.layernorm1=nn.LayerNorm((64, 16, 16))\n",
    "        self.layernorm2=nn.LayerNorm((32, 8, 8))\n",
    "        self.layernorm3=nn.LayerNorm((16, 4, 4))\n",
    "        \n",
    "        self.dropout= nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        #x= self.layernorm1(x)\n",
    "        x = self.BatchNorm32(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        #x = self.BatchNorm16(x)\n",
    "        #x= self.layernorm2(x)\n",
    "        x = self.pool(x)\n",
    "        x=self.dropout(x)\n",
    "        #x = F.leaky_relu(self.conv3(x))\n",
    "        #x = self.BatchNorm8(x)\n",
    "        #x= self.layernorm3(x)\n",
    "        #x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "        x=self.sig(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_epochs = 80\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef=2\n",
    "def my_loss(scores, labels):\n",
    "    idx1=np.argwhere(labels.cpu().numpy()==0).ravel()\n",
    "    idx2=np.argwhere(labels.cpu().numpy()!=0).ravel()\n",
    "    \n",
    "    loss1 = torch.mean((scores[idx1] - labels[idx1])**2)\n",
    "    loss2 = torch.mean((scores[idx2] - labels[idx2])**2)\n",
    "    \n",
    "    if len(idx1)==0:\n",
    "        loss1=0\n",
    "        \n",
    "    if len(idx2)==0:\n",
    "        loss2=0\n",
    "    \n",
    "    \n",
    "    loss= ((loss1*len(idx1))+(coef*loss2*len(idx2)))/(len(idx1)+len(idx2))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device=torch.device(\"cpu\")\n",
    "# Initialize network\n",
    "model = CNN_conv2d().to(device)\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion=nn.MSELoss()\n",
    "#criterion=my_loss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "load_module=False\n",
    "\n",
    "ref=1\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"validation\": test_loader\n",
    "}\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    logs = {}\n",
    "    for phase in ['train', 'validation']:\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "            \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            # Get data to cuda if possible\n",
    "            inputs = inputs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            # forward\n",
    "            inputs=inputs.float()\n",
    "            scores = model(inputs)\n",
    "            labels=labels.float()\n",
    "            scores=scores.view(-1)\n",
    "\n",
    "            loss = my_loss(scores, labels)\n",
    "            loss2 = criterion(scores, labels)            \n",
    "            # backward\n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # gradient descent or adam step\n",
    "                optimizer.step()\n",
    "            preds=torch.round(scores)\n",
    "    \n",
    "            running_loss += loss2.detach() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "        \n",
    "        #if phase == 'validation':\n",
    "        #    this_loss=(running_loss / len(dataloaders[phase].dataset)).item()\n",
    "        #    if this_loss<ref:\n",
    "        #        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        #        # save checkpoint\n",
    "        #        save_checkpoint(checkpoint)\n",
    "        #        ref=this_loss\n",
    "        #        print (\"epoch {}:loss{}\".format(epoch,ref)) \n",
    "        \n",
    "        prefix = ''\n",
    "        if phase == 'validation':\n",
    "            prefix = 'val_'\n",
    "\n",
    "        logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "        logs[prefix + 'log loss again!'] = epoch_loss.item()\n",
    "        #logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        \n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_checkpoint(torch.load(\"my_checkpoint_V3.pth.tar\"), model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MAIN\n",
    "#device = torch.device(\"cpu\")\n",
    "num_epochs = 81\n",
    "# Initialize network\n",
    "\n",
    "model = CNN_conv2d().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion=nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "load_module=False\n",
    "print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\")) \n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    now = datetime.now()\n",
    "    if load_module:\n",
    "        # load checkpoint\n",
    "        load_checkpoint(torch.load(\"my_checkpoint_V3_percent.pth.tar\"), model, optimizer)\n",
    "        load_module=False\n",
    "    losses = []\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        data=data.float()\n",
    "        scores = model(data)\n",
    "        targets=targets.float()\n",
    "        scores=scores.view(-1)\n",
    "\n",
    "        loss = my_loss(scores, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    if (epoch % 20==0):\n",
    "        print(f\"Cost at epoch {epoch} is {np.sum(losses)/len(losses)}\")\n",
    "        print(\"Current Time =\", now.strftime(\"%H:%M:%S\")) \n",
    "        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        # save checkpoint\n",
    "        save_checkpoint(checkpoint)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training to see how good our model is\n",
    "\n",
    "def check_accuracy(loader, model, test_set):\n",
    "    total_mse=0\n",
    "    model.eval()\n",
    "    idx=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x=x.float()\n",
    "            scores = model(x)\n",
    "            predictions=scores\n",
    "            \n",
    "            MSE=mean_squared_error(np.ravel(predictions.cpu().numpy()),np.ravel(y.cpu().numpy()))\n",
    "            total_mse=total_mse+MSE\n",
    "            idx=idx+1\n",
    "            if test_set:\n",
    "                \n",
    "                np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "                print (\"labels:       \",np.ravel(y.cpu().numpy()))\n",
    "                print (\"predictions:  \",np.ravel((predictions.cpu().numpy())))\n",
    "                print (\"mean_squared_error\",MSE)\n",
    "                #print (\"r2_score\",r2_score(np.ravel(predictions.cpu().numpy()),np.ravel(y.cpu().numpy())))\n",
    "\n",
    "    print (\"total_mse\",total_mse/idx)\n",
    "        \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on Training Set\")\n",
    "check_accuracy(train_loader, model ,test_set=False)\n",
    "\n",
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(test_loader, model, test_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(test_loader, model, test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to add radon noise to images\n",
    "\n",
    "PEAK=100\n",
    "def make_noisy_image(image):\n",
    "    img=np.zeros(image.shape)\n",
    "    for idx, _2d_img in enumerate(image):  \n",
    "        \n",
    "        theta = np.linspace(0., 180., max(_2d_img.shape), endpoint=False)\n",
    "        sinogram = radon(_2d_img, theta=theta, circle=False)\n",
    "        sinogram=np.clip(sinogram, 0,None)\n",
    "        noisy = np.random.poisson(sinogram * PEAK)/PEAK  \n",
    "        img[idx] = iradon(noisy, theta=theta, circle=False)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again load all images, add padinf if needed, then add radon noise\n",
    "\n",
    "path = 'D:\\\\Master_courses\\\\Praktikum\\\\challenge\\\\hecktor\\\\data\\\\resampled\\\\'\n",
    "#_ct_gtvt.nii.gz\n",
    "#_pt.nii.gz\n",
    "files = [f for f in glob.glob(path + \"**/*_pt.nii.gz\", recursive=True)]\n",
    "noisy_image_list=list()\n",
    "\n",
    "for idx,f in enumerate(files):\n",
    "    \n",
    "    im=nib.load(f)\n",
    "    PET_data=im.get_fdata()   \n",
    "    if PET_data.shape!=(64,64,64):\n",
    "        PET_data=pad(PET_data,64)\n",
    "    PET_data=make_noisy_image(PET_data)   \n",
    "    noisy_image_list.append(PET_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(noisy_image_list[0][:,:,50],cmap='gray')\n",
    "a = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(image_list[0][:,:,50],cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make image patches of size 16*16*16\n",
    "\n",
    "all_noisy_PET_patches=list()\n",
    "for PET_img in noisy_image_list:\n",
    "    patches = torch.from_numpy(PET_img).unfold(0, size, stride).unfold(1, size, stride).unfold(2, size, stride)\n",
    "    reshaped_patches=patches.reshape(-1,size,size,size).numpy()\n",
    "    all_noisy_PET_patches.append(reshaped_patches)\n",
    "all_noisy_PET_patches=np.asarray(all_noisy_PET_patches).reshape(-1,size,size,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class dataset of noisy images\n",
    "class noisy_HN_Dataset(Dataset):\n",
    "    def __init__(self, transform=tv.transforms.Compose(tv.transforms.ToTensor())):\n",
    "        self.labels = all_labels\n",
    "        self.images=all_noisy_PET_patches\n",
    "        self.transform = transform\n",
    "        self.mode='None'\n",
    "    def __len__(self):\n",
    "        return len(all_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label= torch.from_numpy(np.asarray(self.labels[idx]))        \n",
    "        if self.transform is not None:\n",
    "            PET_data = self.transform(self.images[idx])\n",
    "        return (PET_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making train and test set for noisy image\n",
    "\n",
    "split_threshold=10000\n",
    "idx = list(range(len(all_labels)))\n",
    "\n",
    "noisy_dataset = noisy_HN_Dataset( transform=tv.transforms.ToTensor())\n",
    "\n",
    "################################ IMPOSTANT ############################################\n",
    "#As I don't use \"randome split\" for splitting test and train set, we can easily compare results of this test set with previous one(no noise test set)\n",
    "noisy_train_set = DT.Subset(noisy_dataset, idx[:split_threshold])\n",
    "noisy_test_set = DT.Subset(noisy_dataset, idx[split_threshold:])\n",
    "\n",
    "#noisy_train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "noisy_test_loader = DataLoader(noisy_test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on Test Set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'check_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-c097abaff42e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Checking accuracy on Test Set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoisy_test_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'check_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(noisy_test_loader, model, test_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(noisy_test_loader, model, test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
